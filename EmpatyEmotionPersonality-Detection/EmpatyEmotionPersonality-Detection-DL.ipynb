{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AttahiruJibril\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/WASSA23_conv_level_with_labels_train.tsv'\n",
    "df = pd.read_table(data, header=0)\n",
    "new_col = []\n",
    "for names in df.columns:\n",
    "    new_col.append(names.strip())\n",
    "df.columns = new_col\n",
    "dataset = df.drop([\"conversation_id\", \"turn_id\", \"speaker_number\", \"article_id\", \"speaker_id\", \"essay_id\"], axis=1, inplace=True)\n",
    "\n",
    "X_data, y_data = df.loc[:, 'text'], df.drop('text', axis=1)\n",
    "X_train, X_test, y_train , y_test = train_test_split(X_data, y_data, train_size=0.8)\n",
    "#reset index of training examples\n",
    "X_train, X_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "y_train, y_test = y_train.reset_index(drop=True), y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>EmotionalPolarity</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Empathy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel very sad for the people.               ...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's terrible. Not only the people but the ani...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>3.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I felt really sorry for the sister that now ha...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.6667</td>\n",
       "      <td>2.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yeah, it's going to be tough but i am sure she...</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah, we never know what we can do unless we a...</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>2.3333</td>\n",
       "      <td>1.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771</th>\n",
       "      <td>I'm sure that would go a long way. It's at lea...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8772</th>\n",
       "      <td>Oh exactly, it always comes down to the money....</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8773</th>\n",
       "      <td>And when it's the officials who are getting pa...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8774</th>\n",
       "      <td>Yes corruption I'm sure is very huge. It has t...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8775</th>\n",
       "      <td>I think if the west took more of an interest i...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8776 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  EmotionalPolarity  \\\n",
       "0     I feel very sad for the people.               ...             2.0000   \n",
       "1     It's terrible. Not only the people but the ani...             2.0000   \n",
       "2     I felt really sorry for the sister that now ha...             2.0000   \n",
       "3     Yeah, it's going to be tough but i am sure she...             0.6667   \n",
       "4     Yeah, we never know what we can do unless we a...             0.3333   \n",
       "...                                                 ...                ...   \n",
       "8771  I'm sure that would go a long way. It's at lea...             2.0000   \n",
       "8772  Oh exactly, it always comes down to the money....             0.0000   \n",
       "8773  And when it's the officials who are getting pa...             2.0000   \n",
       "8774  Yes corruption I'm sure is very huge. It has t...             2.0000   \n",
       "8775  I think if the west took more of an interest i...             1.0000   \n",
       "\n",
       "      Emotion  Empathy  \n",
       "0      3.0000   3.3333  \n",
       "1      4.0000   3.3333  \n",
       "2      3.6667   2.6667  \n",
       "3      3.0000   2.0000  \n",
       "4      2.3333   1.3333  \n",
       "...       ...      ...  \n",
       "8771   3.0000   3.0000  \n",
       "8772   2.0000   2.0000  \n",
       "8773   2.0000   2.0000  \n",
       "8774   3.0000   3.0000  \n",
       "8775   2.0000   3.0000  \n",
       "\n",
       "[8776 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenization\n",
    "- remove stop word and punctuatuons, numbers\n",
    "- lematization\n",
    "- vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_preprocessor(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations = set(string.punctuation)\n",
    "    lem = WordNetLemmatizer().lemmatize\n",
    "    sentence = word_tokenize(sentence)\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "    sentence = [word for word in sentence if word not in punctuations]\n",
    "    sentence_str = ' '.join(sentence)\n",
    "    sentence = lem(sentence_str)\n",
    "    return sentence #sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.6667, 2.3333, 1.    ],\n",
       "        [1.3333, 2.    , 3.    ],\n",
       "        [1.3333, 2.    , 2.    ],\n",
       "        ...,\n",
       "        [1.    , 1.    , 1.    ],\n",
       "        [2.    , 4.    , 3.3333],\n",
       "        [1.6667, 3.    , 1.6667]]),\n",
       " array([[0.6667, 4.    , 2.    ],\n",
       "        [1.    , 1.    , 0.6667],\n",
       "        [2.    , 2.3333, 2.6667],\n",
       "        ...,\n",
       "        [2.    , 3.3333, 3.6667],\n",
       "        [0.6667, 2.    , 1.    ],\n",
       "        [1.    , 2.    , 1.3333]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.apply(word_preprocessor)\n",
    "X_test = X_test.apply(word_preprocessor)\n",
    "\n",
    "#convert labels to array\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train[['EmotionalPolarity', 'Emotion', 'Empathy']]), np.array(y_test[['EmotionalPolarity', 'Emotion', 'Empathy']])\n",
    "y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=3000, stop_words='english', lowercase=True)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = X_train_vec.toarray()\n",
    "X_test_vec = X_test_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "71/71 [==============================] - 3s 14ms/step - loss: 2.9720 - accuracy: 0.5607 - val_loss: 2.8817 - val_accuracy: 0.5700\n",
      "Epoch 2/20\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 2.9274 - accuracy: 0.6170 - val_loss: 2.8825 - val_accuracy: 0.5609\n",
      "Epoch 3/20\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 2.9072 - accuracy: 0.6736 - val_loss: 2.8871 - val_accuracy: 0.5478\n",
      "Epoch 4/20\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 2.8928 - accuracy: 0.7127 - val_loss: 2.8932 - val_accuracy: 0.5444\n",
      "Epoch 5/20\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 2.8822 - accuracy: 0.7420 - val_loss: 2.8953 - val_accuracy: 0.5473\n",
      "Epoch 6/20\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 2.8754 - accuracy: 0.7628 - val_loss: 2.9011 - val_accuracy: 0.5194\n",
      "Epoch 7/20\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 2.8713 - accuracy: 0.7744 - val_loss: 2.9017 - val_accuracy: 0.5239\n",
      "Epoch 8/20\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 2.8676 - accuracy: 0.7816 - val_loss: 2.9020 - val_accuracy: 0.5399\n",
      "Epoch 9/20\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 2.8645 - accuracy: 0.7993 - val_loss: 2.9031 - val_accuracy: 0.5342\n",
      "Epoch 10/20\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 2.8620 - accuracy: 0.8171 - val_loss: 2.9052 - val_accuracy: 0.5308\n",
      "Epoch 11/20\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 2.8597 - accuracy: 0.8251 - val_loss: 2.9051 - val_accuracy: 0.5347\n",
      "Epoch 12/20\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 2.8580 - accuracy: 0.8244 - val_loss: 2.9044 - val_accuracy: 0.5330\n",
      "Epoch 13/20\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 2.8566 - accuracy: 0.8301 - val_loss: 2.9062 - val_accuracy: 0.5308\n",
      "Epoch 14/20\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2.8557 - accuracy: 0.8278 - val_loss: 2.9050 - val_accuracy: 0.5376\n",
      "Epoch 15/20\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2.8546 - accuracy: 0.8368 - val_loss: 2.9061 - val_accuracy: 0.5313\n",
      "Epoch 16/20\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 2.8541 - accuracy: 0.8339 - val_loss: 2.9069 - val_accuracy: 0.5251\n",
      "Epoch 17/20\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 2.8533 - accuracy: 0.8372 - val_loss: 2.9051 - val_accuracy: 0.5325\n",
      "Epoch 18/20\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2.8528 - accuracy: 0.8358 - val_loss: 2.9049 - val_accuracy: 0.5330\n",
      "Epoch 19/20\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 2.8522 - accuracy: 0.8400 - val_loss: 2.9054 - val_accuracy: 0.5359\n",
      "Epoch 20/20\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 2.8515 - accuracy: 0.8407 - val_loss: 2.9058 - val_accuracy: 0.5302\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 2.9058 - accuracy: 0.5302\n",
      "Test loss: 2.905837059020996\n",
      "Test accuracy: 0.5301822423934937\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential model\n",
    "EEPD_Model = Sequential()\n",
    "EEPD_Model.add(Dense(100, activation='relu', input_dim=X_train_vec.shape[1]))\n",
    "EEPD_Model.add(Dense(80, activation='relu'))\n",
    "EEPD_Model.add(Dense(50, activation='relu'))\n",
    "EEPD_Model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "EEPD_Model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "EEPD_Model.fit(X_train_vec, y_train, epochs=20, batch_size=100, validation_data=(X_test_vec, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = EEPD_Model.evaluate(X_test_vec, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7020,), (7020, 3), (1756,), (1756, 3))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
