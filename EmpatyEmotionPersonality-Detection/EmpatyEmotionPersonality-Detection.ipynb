{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/WASSA23_conv_level_with_labels_train.tsv'\n",
    "df = pd.read_table(data, header=0)\n",
    "new_col = []\n",
    "for names in df.columns:\n",
    "    new_col.append(names.strip())\n",
    "df.columns = new_col\n",
    "df.drop([\"conversation_id\", \"turn_id\", \"speaker_number\", \"article_id\", \"speaker_id\", \"essay_id\"], axis=1, inplace=True)\n",
    "\n",
    "X_data, y_data = df.loc[:, 'text'], df.drop('text', axis=1) #df.loc[:,'Emotion']\n",
    "X_train, X_test, y_train , y_test = train_test_split(X_data, y_data, train_size=0.8)\n",
    "#reset index of training examples\n",
    "X_train, X_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "y_train, y_test = y_train.reset_index(drop=True), y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0       those are some crazy statistics               ...\n",
       " 1       Incredibly sad, and such a realization of how ...\n",
       " 2       I think we could all benefit from self suffici...\n",
       " 3       I truly hope something can be done to combat t...\n",
       " 4       Yes, there needs to be a way to do something s...\n",
       "                               ...                        \n",
       " 7015    Well me too with all that came forward.  There...\n",
       " 7016    Yeah, I know people are blaming the movie, but...\n",
       " 7017    These past years there has been too many.     ...\n",
       " 7018    I think it was terrible that there were so man...\n",
       " 7019    I remember hearing he jumped into the water he...\n",
       " Name: text, Length: 7020, dtype: object,\n",
       " 0       bye                                           ...\n",
       " 1       What did you think of the article?            ...\n",
       " 2       It is hard to believe he had all those injurie...\n",
       " 3       Yeah these sites that clickbait these cures ne...\n",
       " 4       Yeah, it said the other two that had injuries ...\n",
       "                               ...                        \n",
       " 1751    Did they pay him off? I thought the kids were ...\n",
       " 1752    what do you think can be done to help the peop...\n",
       " 1753    seriously.  I don't even like going to crowded...\n",
       " 1754    Why do you feel bad for the police?           ...\n",
       " 1755    true cant believe that people are against it  ...\n",
       " Name: text, Length: 1756, dtype: object,\n",
       "       EmotionalPolarity  Emotion  Empathy\n",
       " 0                1.3333   2.3333   1.3333\n",
       " 1                2.0000   3.3333   3.0000\n",
       " 2                0.0000   2.3333   2.6667\n",
       " 3                0.6667   2.3333   3.6667\n",
       " 4                1.6667   1.6667   1.6667\n",
       " ...                 ...      ...      ...\n",
       " 7015             1.6667   2.3333   2.0000\n",
       " 7016             2.0000   1.6667   1.3333\n",
       " 7017             2.0000   2.6667   2.6667\n",
       " 7018             1.6667   3.3333   3.3333\n",
       " 7019             1.3333   2.0000   2.0000\n",
       " \n",
       " [7020 rows x 3 columns],\n",
       "       EmotionalPolarity  Emotion  Empathy\n",
       " 0                1.0000   1.3333   0.6667\n",
       " 1                0.6667   1.3333   0.3333\n",
       " 2                1.3333   2.0000   1.6667\n",
       " 3                1.3333   2.6667   2.0000\n",
       " 4                2.0000   2.0000   2.6667\n",
       " ...                 ...      ...      ...\n",
       " 1751             2.0000   1.6667   1.3333\n",
       " 1752             0.6667   2.3333   1.0000\n",
       " 1753             1.6667   2.6667   1.3333\n",
       " 1754             1.0000   1.6667   1.0000\n",
       " 1755             2.0000   1.6667   2.0000\n",
       " \n",
       " [1756 rows x 3 columns])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenization\n",
    "- remove stop word and punctuatuons, numbers\n",
    "- lematization\n",
    "- vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_preprocessor(sentence):\n",
    "    tok = get_tokenizer(\"basic_english\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations = set(string.punctuation)\n",
    "    lem = WordNetLemmatizer().lemmatize\n",
    "\n",
    "    sentence = tok(sentence)\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "    sentence = [word for word in sentence if word not in punctuations]\n",
    "    sentence_str = ' '.join(sentence)\n",
    "    sentence = lem(sentence_str)\n",
    "    return sentence #sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(word_preprocessor)\n",
    "X_test = X_test.apply(word_preprocessor)\n",
    "\n",
    "#convert labels to array\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train[['EmotionalPolarity', 'Emotion', 'Empathy']]), np.array(y_test[['EmotionalPolarity', 'Emotion', 'Empathy']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7020,), (7020, 3), (1756,), (1756, 3))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanSquaredError: \t 0.38767747301557226 \n",
      "MeanAbsoluteError: \t 0.23008384988087646\n"
     ]
    }
   ],
   "source": [
    "regressor = make_pipeline(  \n",
    "                        TfidfVectorizer(max_features=2048),\n",
    "                        MultiOutputRegressor(Ridge())\n",
    "                    )\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test[0], y_pred[0])\n",
    "print(f'MeanSquaredError: \\t {mse} \\nMeanAbsoluteError: \\t {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.33457107, 2.72398481, 1.63169567],\n",
       "        [1.25178642, 1.694748  , 1.29955085],\n",
       "        [1.95352253, 3.07710957, 3.06809042],\n",
       "        [0.94203286, 1.77813621, 1.91456287],\n",
       "        [1.46044139, 2.29102417, 1.914624  ],\n",
       "        [0.30728377, 1.54681841, 0.97736431],\n",
       "        [1.56066379, 1.97697262, 2.26354768],\n",
       "        [1.19975846, 2.18944368, 2.88791831]]),\n",
       " array([[1.    , 2.6667, 1.3333],\n",
       "        [1.    , 1.3333, 2.    ],\n",
       "        [2.    , 3.3333, 3.    ],\n",
       "        [1.    , 1.3333, 1.3333],\n",
       "        [2.    , 3.3333, 2.3333],\n",
       "        [0.3333, 1.6667, 1.3333],\n",
       "        [1.6667, 2.6667, 2.6667],\n",
       "        [1.6667, 2.3333, 3.6667]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:8], y_test[0:8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
