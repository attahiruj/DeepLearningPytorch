{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from matplotlib import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/WASSA23_conv_level_with_labels_train.tsv'\n",
    "df = pd.read_table(data, header=0)\n",
    "new_col = []\n",
    "for names in df.columns:\n",
    "    new_col.append(names.strip())\n",
    "df.columns = new_col\n",
    "df.drop([\"conversation_id\", \"turn_id\", \"speaker_number\", \"article_id\", \"speaker_id\", \"essay_id\"], axis=1, inplace=True)\n",
    "\n",
    "X_data, y_data = df.loc[:, 'text'], df.drop('text', axis=1) #df.loc[:,'Emotion']\n",
    "X_train, X_test, y_train , y_test = train_test_split(X_data, y_data, train_size=0.8)\n",
    "#reset index of training examples\n",
    "X_train, X_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "y_train, y_test = y_train.reset_index(drop=True), y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0       yeah they'd probably just switch to knives    ...\n",
       " 1       I understood your point.  I just didn't really...\n",
       " 2       it's terrible. i would most likely not anymore...\n",
       " 3       Yea. I find that pacifying to an extent. I don...\n",
       " 4       The whole thing is sad. I wonder what protocol...\n",
       "                               ...                        \n",
       " 7015    Look at all the rules at the airport and the p...\n",
       " 7016    yeah I think they're up there with the worst c...\n",
       " 7017    Yes, it's pretty draining.  I think people wou...\n",
       " 7018    bye                                           ...\n",
       " 7019    that's so crazy to try to wrap my head around....\n",
       " Name: text, Length: 7020, dtype: object,\n",
       " 0       expand on what? how women deserve less pay?   ...\n",
       " 1       I agree with you                              ...\n",
       " 2       true it is amazing to read about              ...\n",
       " 3       It changes how you feel and how you live.   Fo...\n",
       " 4       I really wish I can do more                   ...\n",
       "                               ...                        \n",
       " 1751    I had a nice time chatting with you as well. g...\n",
       " 1752    In saner climes, such companies would be sued ...\n",
       " 1753    Yes, they certainly have a tough go of it. But...\n",
       " 1754    fake news-as Trump himself calls it. I stay aw...\n",
       " 1755    Obnoxious in what sense?                      ...\n",
       " Name: text, Length: 1756, dtype: object,\n",
       "       EmotionalPolarity  Emotion  Empathy\n",
       " 0                1.6667   2.0000   1.0000\n",
       " 1                1.3333   2.6667   1.6667\n",
       " 2                1.6667   2.3333   2.3333\n",
       " 3                1.0000   2.3333   2.6667\n",
       " 4                2.0000   3.3333   2.6667\n",
       " ...                 ...      ...      ...\n",
       " 7015             0.3333   2.0000   1.3333\n",
       " 7016             2.0000   2.0000   2.0000\n",
       " 7017             2.0000   3.0000   2.3333\n",
       " 7018             1.0000   1.0000   0.3333\n",
       " 7019             0.6667   4.0000   2.0000\n",
       " \n",
       " [7020 rows x 3 columns],\n",
       "       EmotionalPolarity  Emotion  Empathy\n",
       " 0                1.6667   1.0000   1.0000\n",
       " 1                0.3333   1.3333   1.0000\n",
       " 2                0.0000   2.3333   3.0000\n",
       " 3                0.0000   2.3333   3.3333\n",
       " 4                1.0000   3.0000   2.3333\n",
       " ...                 ...      ...      ...\n",
       " 1751             0.3333   1.3333   0.6667\n",
       " 1752             1.6667   2.3333   1.3333\n",
       " 1753             1.3333   2.3333   2.0000\n",
       " 1754             1.6667   3.0000   1.0000\n",
       " 1755             1.0000   1.3333   0.6667\n",
       " \n",
       " [1756 rows x 3 columns])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenization\n",
    "- remove stop word and punctuatuons, numbers\n",
    "- lematization\n",
    "- vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_preprocessor(sentence):\n",
    "    tok = get_tokenizer(\"basic_english\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations = set(string.punctuation)\n",
    "    lem = WordNetLemmatizer().lemmatize\n",
    "\n",
    "    sentence = tok(sentence)\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "    sentence = [word for word in sentence if word not in punctuations]\n",
    "    sentence_str = ' '.join(sentence)\n",
    "    sentence = lem(sentence_str)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(word_preprocessor)\n",
    "X_test = X_test.apply(word_preprocessor)\n",
    "\n",
    "#convert labels to array\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train[['EmotionalPolarity', 'Emotion', 'Empathy']]), np.array(y_test[['EmotionalPolarity', 'Emotion', 'Empathy']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0],1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],1))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7020,), (7020, 3), (1756,), (1756, 3))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = []\n",
    "\n",
    "for i in range(y_train.shape[1]):\n",
    "    regressor = make_pipeline(  \n",
    "                        TfidfVectorizer(max_features=128),\n",
    "                        LinearRegression()\n",
    "                    )\n",
    "    regressor.fit(X_train, y_train[:,i])\n",
    "    regressors.append(regressor)\n",
    "    \n",
    "y_preds = [regressor.predict(X_test) for regressor in regressors]\n",
    "y_pred = np.column_stack(y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.6667, 2.    , 1.    ],\n",
       "        [2.    , 3.3333, 2.3333],\n",
       "        [0.    , 2.    , 1.3333],\n",
       "        ...,\n",
       "        [1.6667, 2.6667, 2.3333],\n",
       "        [1.3333, 2.6667, 2.3333],\n",
       "        [0.3333, 1.6667, 2.6667]]),\n",
       " array([[1.27934404, 2.05607   , 1.90572553],\n",
       "        [1.08407758, 2.36075325, 1.67917248],\n",
       "        [1.17731092, 2.67748412, 2.5350846 ],\n",
       "        ...,\n",
       "        [1.54336128, 2.57310631, 2.1721677 ],\n",
       "        [1.02477609, 1.95025561, 1.90657692],\n",
       "        [1.07962897, 2.29965639, 2.35280091]]))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regressor = make_pipeline(  \n",
    "                        TfidfVectorizer(max_features=128),\n",
    "                        LinearRegression()\n",
    "                    )\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
