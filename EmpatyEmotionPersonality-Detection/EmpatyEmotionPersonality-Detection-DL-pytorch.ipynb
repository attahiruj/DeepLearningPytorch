{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AttahiruJibril\\AppData\\Local\\Temp\\ipykernel_2756\\1970053945.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "xy = pd.read_table(data, header=0)\n",
    "new_col = []\n",
    "for names in xy.columns:\n",
    "    new_col.append(names.strip())\n",
    "xy.columns = new_col\n",
    "x, y = xy.loc[:, 'text'], xy.loc[:, ['EmotionalPolarity', 'Emotion', 'Empathy']]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(x), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda y: np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "class EEPD_Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        self.text = torch.tensor(x).to(device)\n",
    "        self.label = torch.tensor(y).to(device)\n",
    "        self.n_samples = x.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/WASSA23_conv_level_with_labels_train.tsv'\n",
    "\n",
    "def yield_tokens(text_data):\n",
    "    for text in text_data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "xy = pd.read_table(data, header=0)\n",
    "new_col = []\n",
    "for names in xy.columns:\n",
    "    new_col.append(names.strip())\n",
    "xy.columns = new_col\n",
    "x, y = xy.loc[:, 'text'], xy.loc[:, ['EmotionalPolarity', 'Emotion', 'Empathy']]\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(x), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "train_x, train_y = [], []\n",
    "for text, label in zip(x, y):\n",
    "    x_tokens = vocab(tokenizer(text))\n",
    "    y_label = np.array(label)\n",
    "    train_x.append(x_tokens)\n",
    "    train_y.append(y_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 8 at dim 1 (got 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/WASSA23_conv_level_with_labels_train.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEEPD_Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# data_iter = iter(dataloader)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# for i in range(2):\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     f,l = next(data_iter)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# f,l\u001b[39;00m\n\u001b[0;32m      9\u001b[0m dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m, in \u001b[0;36mEEPD_Dataset.__init__\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 8 at dim 1 (got 15)"
     ]
    }
   ],
   "source": [
    "data = 'data/WASSA23_conv_level_with_labels_train.tsv'\n",
    "dataset = EEPD_Dataset(train_x, train_y)\n",
    "# dataloader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
    "# data_iter = iter(dataloader)\n",
    "\n",
    "# for i in range(2):\n",
    "#     f,l = next(data_iter)\n",
    "# f,l\n",
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m tokenized_sequences \u001b[38;5;241m=\u001b[39m [tokenizer(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert tokenized sequences to tensors\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m tensor_sequences \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m tokenized_sequences]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Pad sequences\u001b[39;00m\n\u001b[0;32m     22\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m pad_sequence(tensor_sequences, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xy = pd.read_table(data, header=0)\n",
    "new_col = []\n",
    "for names in xy.columns:\n",
    "    new_col.append(names.strip())\n",
    "xy.columns = new_col\n",
    "x, y = xy.loc[:, 'text'], xy.loc[:, ['EmotionalPolarity', 'Emotion', 'Empathy']]\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Tokenize sequences\n",
    "tokenized_sequences = [tokenizer(text) for text in x]\n",
    "\n",
    "# Convert tokenized sequences to tensors\n",
    "tensor_sequences = [torch.tensor(tokens) for tokens in tokenized_sequences]\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequence(tensor_sequences, batch_first=True)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(x), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Convert labels to numpy array\n",
    "y = np.array(y)\n",
    "\n",
    "# Process the data\n",
    "train_x, train_y = [], []\n",
    "max_length = padded_sequences.size(1)  # Get the maximum length after padding\n",
    "\n",
    "for text, label in zip(x, y):\n",
    "    x_tokens = torch.tensor(vocab(tokenizer(text))).to(device)\n",
    "    \n",
    "    # Pad the tokenized sequence to max_length\n",
    "    x_tokens_padded = F.pad(x_tokens, (0, max_length - len(x_tokens)))\n",
    "    \n",
    "    y_label = torch.tensor(label).to(device)\n",
    "    \n",
    "    train_x.append(x_tokens_padded)\n",
    "    train_y.append(y_label)\n",
    "\n",
    "train_x = torch.stack(train_x)\n",
    "train_y = torch.stack(train_y)\n",
    "\n",
    "# Now train_x and train_y are tensors with padded sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def collate_batch(text , label):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    for  _text, _label in text , label:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EEPD_Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        data = 'data/WASSA23_conv_level_with_labels_train.tsv'\n",
    "        xy = pd.read_table(data, header=0)\n",
    "        new_col = []\n",
    "        for names in xy.columns:\n",
    "            new_col.append(names.strip())\n",
    "        xy.columns = new_col\n",
    "        x, y = xy.loc[:, 'text'], xy.loc[:, ['EmotionalPolarity', 'Emotion', 'Empathy']]\n",
    "        self.x = np.array(x)\n",
    "        self.y = torch.from_numpy(np.array(y))\n",
    "        self.n_samples = xy.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 8\n",
    "n_samples = len(dataset)\n",
    "n_iterations = math.ceil(n_samples/4)\n",
    "\n",
    "print(n_samples, n_iterations)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (input, labels) in enumerate(dataloader):\n",
    "        print(f'Epoch: {epoch+1}/{n_epochs} | step{i+1}/{n_iterations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenization\n",
    "- remove stop word and punctuatuons, numbers\n",
    "- lematization\n",
    "- vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_preprocessor(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations = set(string.punctuation)\n",
    "    lem = WordNetLemmatizer().lemmatize\n",
    "    sentence = word_tokenize(sentence)\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "    sentence = [word for word in sentence if word not in punctuations]\n",
    "    sentence_str = ' '.join(sentence)\n",
    "    sentence = lem(sentence_str)\n",
    "    return sentence #sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
