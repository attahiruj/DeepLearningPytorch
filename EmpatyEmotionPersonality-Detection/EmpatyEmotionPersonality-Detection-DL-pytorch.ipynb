{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "class EEPD_Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        data = 'data/WASSA23_conv_level_with_labels_train.tsv'\n",
    "\n",
    "        xy = pd.read_table(data, header=0)\n",
    "        new_col = []\n",
    "        for names in xy.columns:\n",
    "            new_col.append(names.strip())\n",
    "        xy.columns = new_col\n",
    "        x, y = xy.loc[:, 'text'], xy.loc[:, ['EmotionalPolarity', 'Emotion', 'Empathy']]\n",
    "        y = np.array(y)\n",
    "\n",
    "\n",
    "        padd_to_tensor = torchtext.transforms.ToTensor(padding_value=0)\n",
    "        vocab = build_vocab_from_iterator(self.yield_tokens(x), specials=[\"<unk>\"])\n",
    "        vocab.set_default_index(vocab[\"<unk>\"])\n",
    "        train_x, train_y = [], []\n",
    "        for text, label in zip(x, y):\n",
    "            x_tokens = vocab(tokenizer(text))\n",
    "            y_label = np.array(label)\n",
    "            train_x.append(x_tokens)\n",
    "            train_y.append(y_label)\n",
    "        train_x = padd_to_tensor(train_x)\n",
    "        \n",
    "        self.text = torch.tensor(train_x, dtype=torch.float32).to(device)\n",
    "        self.label = torch.tensor(train_y).to(device)\n",
    "        self.n_samples = x.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def yield_tokens(self, text_data):\n",
    "        for text in text_data:\n",
    "            yield tokenizer(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_classes):\n",
    "        super(DeepNN,self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l3 = nn.Linear(hidden_size, output_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 7021 | n_iterations/batch: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AttahiruJibril\\AppData\\Local\\Temp\\ipykernel_6172\\2203063665.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.text = torch.tensor(train_x, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "dataset = EEPD_Dataset()\n",
    "#train, test split\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, test_set = random_split(dataset=dataset, lengths=[0.8, 0.2], generator=generator)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_samples = len(train_set)\n",
    "n_epochs = 32\n",
    "n_input = len(train_set[0][0])\n",
    "n_hidden = 100\n",
    "n_classes = 3\n",
    "n_batch = 64\n",
    "n_iterations = math.ceil(n_samples/n_batch)\n",
    "\n",
    "print(f'n_samples: {n_samples} | n_iterations/batch: {n_iterations}')\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_set, batch_size=n_batch, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(train_dataloader)\n",
    "\n",
    "# for i in range(1):\n",
    "#     feature,label = next(data_iter)\n",
    "# feature,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(test_dataloader)\n",
    "x, y = next(data_iter)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "DeepNN                                   --\n",
      "├─Linear: 1-1                            13,400\n",
      "├─ReLU: 1-2                              --\n",
      "├─Linear: 1-3                            10,100\n",
      "├─Linear: 1-4                            303\n",
      "=================================================================\n",
      "Total params: 23,803\n",
      "Trainable params: 23,803\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "epoch: 1/32 | step: 100/110 | loss: 6.3765\n",
      "epoch: 2/32 | step: 100/110 | loss: 5.9431\n",
      "epoch: 3/32 | step: 100/110 | loss: 6.2992\n",
      "epoch: 4/32 | step: 100/110 | loss: 6.4690\n",
      "epoch: 5/32 | step: 100/110 | loss: 8.1553\n",
      "epoch: 6/32 | step: 100/110 | loss: 6.3326\n",
      "epoch: 7/32 | step: 100/110 | loss: 7.8228\n",
      "epoch: 8/32 | step: 100/110 | loss: 6.8815\n",
      "epoch: 9/32 | step: 100/110 | loss: 9.0216\n",
      "epoch: 10/32 | step: 100/110 | loss: 7.8409\n",
      "epoch: 11/32 | step: 100/110 | loss: 9.4121\n",
      "epoch: 12/32 | step: 100/110 | loss: 8.3132\n",
      "epoch: 13/32 | step: 100/110 | loss: 7.7587\n",
      "epoch: 14/32 | step: 100/110 | loss: 12.4483\n",
      "epoch: 15/32 | step: 100/110 | loss: 7.4225\n",
      "epoch: 16/32 | step: 100/110 | loss: 9.3493\n",
      "epoch: 17/32 | step: 100/110 | loss: 11.1852\n",
      "epoch: 18/32 | step: 100/110 | loss: 5.9364\n",
      "epoch: 19/32 | step: 100/110 | loss: 9.3289\n",
      "epoch: 20/32 | step: 100/110 | loss: 12.4853\n",
      "epoch: 21/32 | step: 100/110 | loss: 12.8494\n",
      "epoch: 22/32 | step: 100/110 | loss: 10.5682\n",
      "epoch: 23/32 | step: 100/110 | loss: 7.4609\n",
      "epoch: 24/32 | step: 100/110 | loss: 7.8304\n",
      "epoch: 25/32 | step: 100/110 | loss: 12.1708\n",
      "epoch: 26/32 | step: 100/110 | loss: 12.0639\n",
      "epoch: 27/32 | step: 100/110 | loss: 7.5700\n",
      "epoch: 28/32 | step: 100/110 | loss: 9.6233\n",
      "epoch: 29/32 | step: 100/110 | loss: 14.2883\n",
      "epoch: 30/32 | step: 100/110 | loss: 7.4618\n",
      "epoch: 31/32 | step: 100/110 | loss: 8.9533\n",
      "epoch: 32/32 | step: 100/110 | loss: 13.3579\n"
     ]
    }
   ],
   "source": [
    "model = DeepNN(n_input, n_hidden, n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(summary(model, batch_dim=n_batch))\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (input, label) in enumerate(train_dataloader):\n",
    "        #forward\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch: {epoch+1}/{n_epochs} | step: {i+1}/{n_iterations} | loss: {loss.item():.4f}')\n",
    "    \n",
    "#test\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i, (input, label) in enumerate(train_dataloader):\n",
    "        outputs = model(input)\n",
    "        # _, predictions = torch.max(outputs, 1)\n",
    "        # n_samples += label.shape[0]\n",
    "    #     n_correct = (predictions == label).sum().item()\n",
    "    # accuracy = 100.0 * n_correct /n_samples\n",
    "    # print(f'accuracy: {accuracy}')\n",
    "    # print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
